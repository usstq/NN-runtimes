{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with OpenVINO in jupyter notebook\n",
    "\n",
    "Enviroment setup is done as described in [here](./enable_openvino_in_jupyter.md)\n",
    "\n",
    " - C++: src/inference/src/ie_core.cpp\n",
    " - Python: src/bindings/python/src/pyopenvino/core/core.cpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CPU</td>\n",
       "      <td>openvino_intel_cpu_plugin</td>\n",
       "      <td>2022.1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...</td>\n",
       "      <td>SUPPORTED_PROPERTIES</td>\n",
       "      <td>RO</td>\n",
       "      <td>{'SUPPORTED_PROPERTIES': 'RO', 'AVAILABLE_DEVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...</td>\n",
       "      <td>AVAILABLE_DEVICES</td>\n",
       "      <td>RO</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...</td>\n",
       "      <td>RANGE_FOR_ASYNC_INFER_REQUESTS</td>\n",
       "      <td>RO</td>\n",
       "      <td>(1, 1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>...</td>\n",
       "      <td>RANGE_FOR_STREAMS</td>\n",
       "      <td>RO</td>\n",
       "      <td>(1, 88)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>...</td>\n",
       "      <td>FULL_DEVICE_NAME</td>\n",
       "      <td>RO</td>\n",
       "      <td>Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>...</td>\n",
       "      <td>OPTIMIZATION_CAPABILITIES</td>\n",
       "      <td>RO</td>\n",
       "      <td>[FP32, FP16, INT8, BIN, IMPORT_EXPORT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>...</td>\n",
       "      <td>NUM_STREAMS</td>\n",
       "      <td>RW</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>...</td>\n",
       "      <td>AFFINITY</td>\n",
       "      <td>RW</td>\n",
       "      <td>(ERROR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>...</td>\n",
       "      <td>INFERENCE_NUM_THREADS</td>\n",
       "      <td>RW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>...</td>\n",
       "      <td>PERF_COUNT</td>\n",
       "      <td>RW</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>...</td>\n",
       "      <td>INFERENCE_PRECISION_HINT</td>\n",
       "      <td>RW</td>\n",
       "      <td>(ERROR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>...</td>\n",
       "      <td>PERFORMANCE_HINT</td>\n",
       "      <td>RW</td>\n",
       "      <td>(ERROR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>...</td>\n",
       "      <td>PERFORMANCE_HINT_NUM_REQUESTS</td>\n",
       "      <td>RW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                               1       2  \\\n",
       "0   CPU       openvino_intel_cpu_plugin  2022.1   \n",
       "1   ...            SUPPORTED_PROPERTIES      RO   \n",
       "2   ...               AVAILABLE_DEVICES      RO   \n",
       "3   ...  RANGE_FOR_ASYNC_INFER_REQUESTS      RO   \n",
       "4   ...               RANGE_FOR_STREAMS      RO   \n",
       "5   ...                FULL_DEVICE_NAME      RO   \n",
       "6   ...       OPTIMIZATION_CAPABILITIES      RO   \n",
       "7   ...                     NUM_STREAMS      RW   \n",
       "8   ...                        AFFINITY      RW   \n",
       "9   ...           INFERENCE_NUM_THREADS      RW   \n",
       "10  ...                      PERF_COUNT      RW   \n",
       "11  ...        INFERENCE_PRECISION_HINT      RW   \n",
       "12  ...                PERFORMANCE_HINT      RW   \n",
       "13  ...   PERFORMANCE_HINT_NUM_REQUESTS      RW   \n",
       "\n",
       "                                                    3  \n",
       "0                                                None  \n",
       "1   {'SUPPORTED_PROPERTIES': 'RO', 'AVAILABLE_DEVI...  \n",
       "2                                                  []  \n",
       "3                                           (1, 1, 1)  \n",
       "4                                             (1, 88)  \n",
       "5           Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz  \n",
       "6              [FP32, FP16, INT8, BIN, IMPORT_EXPORT]  \n",
       "7                                                   1  \n",
       "8                                             (ERROR)  \n",
       "9                                                   0  \n",
       "10                                              False  \n",
       "11                                            (ERROR)  \n",
       "12                                            (ERROR)  \n",
       "13                                                  0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openvino.preprocess import PrePostProcessor\n",
    "import openvino.runtime as ov\n",
    "import numpy as np\n",
    "import time\n",
    "from pandas import DataFrame\n",
    "from graphviz import Digraph, Source\n",
    "import ovhelper\n",
    "\n",
    "# this is top object\n",
    "core = ov.Core()\n",
    "\n",
    "proplist = []\n",
    "\n",
    "for id, dev in enumerate(core.available_devices):\n",
    "    v=core.get_versions(dev)[dev]\n",
    "    proplist.append([dev, v.description, \"{}.{}\".format(v.major, v.minor, v.build_number)])\n",
    "    SUPPORTED_PROPERTIES = core.get_property(dev, \"SUPPORTED_PROPERTIES\")\n",
    "    for prop, rw in SUPPORTED_PROPERTIES.items():\n",
    "        try:\n",
    "            value = core.get_property(dev, prop)\n",
    "        except:\n",
    "            value = \"(ERROR)\"\n",
    "        proplist.append([\"...\", prop, rw, value])\n",
    "df = DataFrame(proplist)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description API\n",
    "\n",
    "ov.op/opset8/Model is for describing model using python. but this is not typical usage since openvino do not train NN.\n",
    "\n",
    " - src/bindings/python/src/pyopenvino/graph/node.cpp\n",
    " - src/core/src/model.cpp\n",
    " - src/bindings/python/src/pyopenvino/graph/model.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_node: <Parameter: 'Parameter_3859' ({2,3,7,7}, float)>\n",
      "conv_node:<Convolution: 'Convolution_3861' ({2,1,7,7})>\n",
      "maxpool_node:<MaxPool: 'MaxPool_3862' ({2,1,2,2})>\n",
      "<Model: 'cnntest'\n",
      "inputs[\n",
      "<ConstOutput: names[Parameter_3859] shape{2,3,7,7} type: f32>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[] shape{8} type: f32>\n",
      "]>\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "input_shape = [2, 3, 7, 7]\n",
    "param_node = ov.op.Parameter(ov.Type.f32, ov.Shape(input_shape))\n",
    "print('param_node: {}'.format(param_node))\n",
    "\n",
    "# convolution weights in shape [out_channels, in_channels, kernel_height, kernel_width]\n",
    "padding_begin = padding_end = [0, 0]\n",
    "conv_kernel = ov.op.Constant(ov.Type.f32, ov.Shape([1,3,1,1]), np.ones([3]))\n",
    "conv_node = ov.opset8.convolution(param_node, conv_kernel, [1, 1], padding_begin, padding_end, [1, 1])\n",
    "\n",
    "# we can see shape inference happens immediatly\n",
    "print('conv_node:{}'.format(conv_node))\n",
    "\n",
    "maxpool_node = ov.opset1.max_pool(conv_node, [4, 4], padding_begin, padding_end, [4, 4], 'ceil')\n",
    "print('maxpool_node:{}'.format(maxpool_node))\n",
    "\n",
    "relu_node = ov.opset8.relu(maxpool_node)\n",
    "reshape_node = ov.opset8.reshape(relu_node, [-1], False)\n",
    "model = ov.Model(reshape_node, [param_node], 'cnntest')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple piece of code for examining the model in a human-friendly way, just like `onnx.helper.printable_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(Parameter_3859):\n",
      "    Tensor<2x3x7x7xf32> t1 = Parameter_3859(element_type=f32)\n",
      "    Tensor<1x3x1x1xf32> t2 = Constant_3860(element_type=f32,shape=[1, 3, 1, 1])\n",
      "    Tensor<2x1x7x7xf32> t3 = Convolution_3861(t1,t2,strides=[1, 1],dilations=[1, 1],pads_begin=[0, 0],pads_end=[0, 0],auto_pad=explicit)\n",
      "    Tensor<2x1x2x2xf32> t4 = MaxPool_3862(t3,strides=[4, 4],pads_begin=[0, 0],pads_end=[0, 0],kernel=[4, 4],rounding_type=ceil,auto_pad=explicit)\n",
      "    Tensor<2x1x2x2xf32> t5 = Relu_3863(t4)\n",
      "    Tensor<1xi64> t6 = Constant_3864(element_type=i64,shape=[1])\n",
      "    Tensor<8xf32> t7 = Reshape_3865(t5,t6,special_zero=False)\n",
      "    Tensor<8xf32> t8 = Result_3871(t7)\n",
      "    return t8\n"
     ]
    }
   ],
   "source": [
    "ovhelper.print_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The psuedocode above essentially contains all topology/configuration information about a model,but visualize with graphviz gives even superior representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example_model1.svg'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ovhelper.visualize_model(model, filename=\"example_model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example_model1](./example_model1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can hover your mouse on nodes to see details about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `Model` here is high-level device-independent representation of CNN in openvino, most of the time user get this by converting from models of other NN frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model convert(import) APIs\n",
    "\n",
    "src/bindings/python/src/pyopenvino/core/core.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check supportiveness of a Model\n",
    "\n",
    "Usually user just call compile_model() on a device w/o this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant_3860 is supported by CPU\n",
      "Constant_3864 is supported by CPU\n",
      "Convolution_3861 is supported by CPU\n",
      "MaxPool_3862 is supported by CPU\n",
      "Parameter_3859 is supported by CPU\n",
      "Relu_3863 is supported by CPU\n",
      "Reshape_3865 is supported by CPU\n",
      "Result_3871 is supported by CPU\n"
     ]
    }
   ],
   "source": [
    "# returns  Pairs a operation name -> a device name supporting this operation.\n",
    "for opname, devname in core.query_model(model, 'CPU').items():\n",
    "    print(\"{} is supported by {}\".format(opname, devname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model on particular device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported model is saved to  /home/hddl/NN-runtimes/OpenVINO/my_model\n",
      "=============runtime model=============\n",
      "model(Parameter_6426):\n",
      "    Tensor<2x3x7x7xf32> t1 = Parameter_3859(element_type=f32)\n",
      "\t\t\t#rt_info:\n",
      "\t\t\t#\texecOrder=3\n",
      "\t\t\t#\texecTimeMcs=not_executed\n",
      "\t\t\t#\tlayerType=Input\n",
      "\t\t\t#\toriginalLayersNames=Parameter_3859\n",
      "\t\t\t#\toutputLayouts=abcd\n",
      "\t\t\t#\toutputPrecisions=FP32\n",
      "\t\t\t#\tprimitiveType=unknown_FP32\n",
      "\t\t\t#\truntimePrecision=FP32\n",
      "\n",
      "    Tensor<1x3x1x1xf32> t2 = Constant_3860()\n",
      "\t\t\t#rt_info:\n",
      "\t\t\t#\texecOrder=1\n",
      "\t\t\t#\texecTimeMcs=not_executed\n",
      "\t\t\t#\tlayerType=Const\n",
      "\t\t\t#\toriginalLayersNames=Constant_3860\n",
      "\t\t\t#\toutputLayouts=abcd\n",
      "\t\t\t#\toutputPrecisions=FP32\n",
      "\t\t\t#\tprimitiveType=unknown_FP32\n",
      "\t\t\t#\truntimePrecision=FP32\n",
      "\n",
      "    Tensor<1x3x1x1xf32> t3 = Constant_3860_abcd_Acdb8a_Convolution_3861(t2)\n",
      "\t\t\t#rt_info:\n",
      "\t\t\t#\texecOrder=2\n",
      "\t\t\t#\texecTimeMcs=not_executed\n",
      "\t\t\t#\tlayerType=Reorder\n",
      "\t\t\t#\toriginalLayersNames=\n",
      "\t\t\t#\toutputLayouts=Acdb8a\n",
      "\t\t\t#\toutputPrecisions=FP32\n",
      "\t\t\t#\tprimitiveType=ref_any_FP32\n",
      "\t\t\t#\truntimePrecision=FP32\n",
      "\n",
      "    Tensor<2x1x7x7xf32> t4 = Convolution_3861(t1,t3)\n",
      "\t\t\t#rt_info:\n",
      "\t\t\t#\texecOrder=4\n",
      "\t\t\t#\texecTimeMcs=not_executed\n",
      "\t\t\t#\tlayerType=Convolution\n",
      "\t\t\t#\toriginalLayersNames=Convolution_3861,Relu_3863\n",
      "\t\t\t#\toutputLayouts=aBcd8b\n",
      "\t\t\t#\toutputPrecisions=FP32\n",
      "\t\t\t#\tprimitiveType=jit_avx2_FP32\n",
      "\t\t\t#\truntimePrecision=FP32\n",
      "\n",
      "    Tensor<2x1x7x7xf32> t5 = Convolution_3861_aBcd8b_abcd_MaxPool_3862(t4)\n",
      "\t\t\t#rt_info:\n",
      "\t\t\t#\texecOrder=5\n",
      "\t\t\t#\texecTimeMcs=not_executed\n",
      "\t\t\t#\tlayerType=Reorder\n",
      "\t\t\t#\toriginalLayersNames=\n",
      "\t\t\t#\toutputLayouts=abcd\n",
      "\t\t\t#\toutputPrecisions=FP32\n",
      "\t\t\t#\tprimitiveType=ref_any_FP32\n",
      "\t\t\t#\truntimePrecision=FP32\n",
      "\n",
      "    Tensor<2x1x2x2xf32> t6 = MaxPool_3862(t5)\n",
      "\t\t\t#rt_info:\n",
      "\t\t\t#\texecOrder=6\n",
      "\t\t\t#\texecTimeMcs=not_executed\n",
      "\t\t\t#\tlayerType=Pooling\n",
      "\t\t\t#\toriginalLayersNames=MaxPool_3862\n",
      "\t\t\t#\toutputLayouts=abcd\n",
      "\t\t\t#\toutputPrecisions=FP32\n",
      "\t\t\t#\tprimitiveType=ref_any_FP32\n",
      "\t\t\t#\truntimePrecision=FP32\n",
      "\n",
      "    Tensor<1xi32> t7 = Constant_3864()\n",
      "\t\t\t#rt_info:\n",
      "\t\t\t#\texecOrder=0\n",
      "\t\t\t#\texecTimeMcs=not_executed\n",
      "\t\t\t#\tlayerType=Const\n",
      "\t\t\t#\toriginalLayersNames=Constant_3864\n",
      "\t\t\t#\toutputLayouts=a\n",
      "\t\t\t#\toutputPrecisions=I32\n",
      "\t\t\t#\tprimitiveType=unknown_I32\n",
      "\t\t\t#\truntimePrecision=I32\n",
      "\n",
      "    Tensor<8xf32> t8 = Reshape_3865(t6,t7)\n",
      "\t\t\t#rt_info:\n",
      "\t\t\t#\texecOrder=7\n",
      "\t\t\t#\texecTimeMcs=not_executed\n",
      "\t\t\t#\tlayerType=Reshape\n",
      "\t\t\t#\toriginalLayersNames=Reshape_3865\n",
      "\t\t\t#\toutputLayouts=a\n",
      "\t\t\t#\toutputPrecisions=FP32\n",
      "\t\t\t#\tprimitiveType=unknown_FP32\n",
      "\t\t\t#\truntimePrecision=FP32\n",
      "\n",
      "    Tensor<8xf32> t9 = Result_3871(t8)\n",
      "\t\t\t#rt_info:\n",
      "\t\t\t#\texecOrder=8\n",
      "\t\t\t#\texecTimeMcs=not_executed\n",
      "\t\t\t#\tlayerType=Output\n",
      "\t\t\t#\toriginalLayersNames=Result_3871\n",
      "\t\t\t#\toutputLayouts=undef\n",
      "\t\t\t#\toutputPrecisions=FP32\n",
      "\t\t\t#\tprimitiveType=unknown_FP32\n",
      "\t\t\t#\truntimePrecision=FP32\n",
      "\n",
      "    return t9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'example_runtime_model1.svg'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'CPU'\n",
    "# enable PERF_COUNT\n",
    "core.set_property(device, {\"PERF_COUNT\": \"YES\"})\n",
    "compiled_model = core.compile_model(model, device)\n",
    "\n",
    "# exported model is actually the IR in xml format\n",
    "user_stream = compiled_model.export_model()\n",
    "with open('./my_model', 'wb') as f:\n",
    "    f.write(user_stream)\n",
    "\n",
    "!echo \"exported model is saved to \" `realpath ./my_model`\n",
    "\n",
    "# import_model also returns a compiled model\n",
    "#core.import_model(\"./my_model\",'CPU')\n",
    "\n",
    "# runtime model is internal low-level executable graph for particular device\n",
    "# although still a ngraph function, but it uses different set of op (`ExecutionNode`)\n",
    "# to represent internal OP, and all useful information is in get_rt_info()\n",
    "print(\"=============runtime model=============\")\n",
    "runtime_model = compiled_model.get_runtime_model()\n",
    "ovhelper.print_model(runtime_model)\n",
    "ovhelper.visualize_model(runtime_model, filename=\"example_runtime_model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example_runtime_model1](./example_runtime_model1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CPU plugin, the runtime model has additional runtime information embedded:\n",
    " - layerType: the exact operation type\n",
    " - outputLayouts: define the output tensor memory layout.\n",
    " - outputPrecisions: the runtime precision\n",
    " - primitiveType: the optimization/implementation type\n",
    " - execTimeMcs: execution time (same as profiling_info)\n",
    "\n",
    "Graphviz visualization integrated these additional information to allow us to gain some insights into CPU plugin runtime model:\n",
    "  - All ops are mapped from CPU plugin internal node(like Pooling/Input/Output/...)\n",
    "  - Relu exchanged with Maxpooling and finally fused into \n",
    "  - Convolution is done using blocked memory format\n",
    "  - Reorder node is inserted to convert weights into blocked format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The APIs and relationships can be visualized as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_api.svg'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Digraph(\"openvino models relationship\")\n",
    "g.node(name='Model')\n",
    "g.node(name='CompiledModel')\n",
    "g.node(name='IR')\n",
    "g.node(name='IR/ONNX/PDPD')\n",
    "g.node(name='Model(Runtime)')\n",
    "\n",
    "g.edge('Model','CompiledModel',label='compile_model()', color='blue')\n",
    "g.edge('CompiledModel','IR',label='.export_model()', color='red')\n",
    "g.edge('IR/ONNX/PDPD','CompiledModel',label='compile_model()', color='blue')\n",
    "g.edge('IR/ONNX/PDPD','Model',label='read_model()', color='blue')\n",
    "g.edge('IR','CompiledModel',label='import_model()', color='blue')\n",
    "g.edge('CompiledModel','Model(Runtime)',label='.get_runtime_model()', color='red')\n",
    "\n",
    "dots = Source(g, format=\"svg\")\n",
    "dots.render(\"model_api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![model_api.svg](./model_api.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    " - src/bindings/python/src/pyopenvino/core/compiled_model.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check input/output information of the model\n",
    "\n",
    " - src/bindings/python/src/pyopenvino/graph/node_output.hpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[0]: <ConstOutput: names[Parameter_3859] shape{2,3,7,7} type: f32>\n",
      "outputs[0]: <ConstOutput: names[] shape{8} type: f32>\n"
     ]
    }
   ],
   "source": [
    "# check inputs & outputs (they are actually graph op)\n",
    "# these are `ov::Output<const ov::Node>` type internally\n",
    "for k, input in enumerate(compiled_model.inputs):\n",
    "    print(\"inputs[{}]: {}\".format(k,input))\n",
    "for k, output in enumerate(compiled_model.outputs):\n",
    "    print(\"outputs[{}]: {}\".format(k,output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronously:  CompiledModel.infer_new_request()\n",
    "\n",
    "Steps:\n",
    " - Create input `ov.Tensor`: src/bindings/python/src/pyopenvino/core/tensor.\n",
    " - Feed data into tensor\n",
    " - invoke `infer_new_request()`\n",
    "\n",
    " This is low-performance API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<ConstOutput: names[] shape{8} type: f32>: array([3., 3., 3., 3., 6., 6., 6., 6.], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# get output description of the model\n",
    "inode = compiled_model.input(0)\n",
    "\n",
    "# numpy array can also be input of infer_new_request()\n",
    "# but Tensor is best since it understands element_type while numpy don't\n",
    "input = ov.Tensor(inode.get_element_type(), inode.get_shape())\n",
    "\n",
    "# the data member is numpy array wrapper of the tensor\n",
    "# so we can manipulate data using numpy\n",
    "input.data[0,:,:,:] = 1\n",
    "input.data[1,:,:,:] = 2\n",
    "\n",
    "# infer_new_request is synchronous API\n",
    "# the key in input dict can be int/string/opNode\n",
    "param_name = list(compiled_model.input(0).names)[0]\n",
    "result = compiled_model.infer_new_request({0:input})\n",
    "result = compiled_model.infer_new_request({param_name:input})\n",
    "result = compiled_model.infer_new_request({compiled_model.input(0):input})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronously:  infer_request/async_infer_queue\n",
    "\n",
    " - src/inference/src/cpp/ie_infer_request.cpp\n",
    " - src/bindings/python/src/pyopenvino/core/infer_request.cpp\n",
    " - src/bindings/python/src/pyopenvino/core/async_infer_queue.cpp\n",
    "\n",
    "This concept of infer_request is important:\n",
    " - it has input/output memory allocated by default, user can get a tensor wrapper around them and accessing the memory directly using:\n",
    "    - `get_input_tensor/get_output_tensor`: indexed by port number\n",
    "    - `get_tensor`: indexed by name/port_description/    (internal API: GetBlob)\n",
    " - it also allows users to provide their own memory as desired input/output location:\n",
    "   - `set_input_tensor/set_output_tensor`: indexed by port number\n",
    "   - `set_tensor`: indexed by name/port_description/     (internal API: SetBlob)\n",
    " - it can be triggered in both sync/async way(with optional new input tensor provided), and set call back:\n",
    "   - `start_async`: async\n",
    "   - `infer`:   sync\n",
    " - it tracks latest latency internally:\n",
    "   - `latency`: property\n",
    " - it provides profiling information for each layer:\n",
    "   - `profiling_info`: property (internal API: GetPerformanceCounts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callback received output tensor:[24. 24. 24. 24. 24. 24. 24. 24.]\n",
      "callback received output tensor:[3. 3. 3. 3. 3. 3. 3. 3.]\n",
      "callback received output tensor:[6. 6. 6. 6. 6. 6. 6. 6.]\n",
      "{<ConstOutput: names[] shape{8} type: f32>: array([9., 9., 9., 9., 9., 9., 9., 9.], dtype=float32)}\n",
      "[9. 9. 9. 9. 9. 9. 9. 9.]\n",
      "latency:0.293147 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_name</th>\n",
       "      <th>node_type</th>\n",
       "      <th>exec_type</th>\n",
       "      <th>status</th>\n",
       "      <th>real_time(us)</th>\n",
       "      <th>cpu_time(us)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Parameter_3859</td>\n",
       "      <td>Parameter</td>\n",
       "      <td>unknown_FP32</td>\n",
       "      <td>Status.NOT_RUN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Convolution_3861</td>\n",
       "      <td>Convolution</td>\n",
       "      <td>jit_avx2_FP32</td>\n",
       "      <td>Status.EXECUTED</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relu_3863</td>\n",
       "      <td>Relu</td>\n",
       "      <td>undef</td>\n",
       "      <td>Status.NOT_RUN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Convolution_3861_aBcd8b_abcd_MaxPool_3862</td>\n",
       "      <td>Reorder</td>\n",
       "      <td>ref_any_FP32</td>\n",
       "      <td>Status.EXECUTED</td>\n",
       "      <td>89.0</td>\n",
       "      <td>89.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MaxPool_3862</td>\n",
       "      <td>MaxPool</td>\n",
       "      <td>ref_any_FP32</td>\n",
       "      <td>Status.EXECUTED</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reshape_3865</td>\n",
       "      <td>Reshape</td>\n",
       "      <td>unknown_FP32</td>\n",
       "      <td>Status.NOT_RUN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Result_3871</td>\n",
       "      <td>Result</td>\n",
       "      <td>unknown_FP32</td>\n",
       "      <td>Status.NOT_RUN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   node_name    node_type      exec_type  \\\n",
       "0                             Parameter_3859    Parameter   unknown_FP32   \n",
       "1                           Convolution_3861  Convolution  jit_avx2_FP32   \n",
       "2                                  Relu_3863         Relu          undef   \n",
       "3  Convolution_3861_aBcd8b_abcd_MaxPool_3862      Reorder   ref_any_FP32   \n",
       "4                               MaxPool_3862      MaxPool   ref_any_FP32   \n",
       "5                               Reshape_3865      Reshape   unknown_FP32   \n",
       "6                                Result_3871       Result   unknown_FP32   \n",
       "\n",
       "            status  real_time(us)  cpu_time(us)  \n",
       "0   Status.NOT_RUN            0.0           0.0  \n",
       "1  Status.EXECUTED           52.0          52.0  \n",
       "2   Status.NOT_RUN            0.0           0.0  \n",
       "3  Status.EXECUTED           89.0          89.0  \n",
       "4  Status.EXECUTED           22.0          22.0  \n",
       "5   Status.NOT_RUN            0.0           0.0  \n",
       "6   Status.NOT_RUN            0.0           0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_request = compiled_model.create_infer_request()\n",
    "def callback1(irq):\n",
    "    print(\"callback received output tensor:{}\".format(irq.get_output_tensor(0).data))\n",
    "\n",
    "infer_request.set_callback(callback1, infer_request)\n",
    "\n",
    "# use input/output memory provided by infer_request\n",
    "itensor = infer_request.get_input_tensor(0)\n",
    "otensor = infer_request.get_output_tensor(0)\n",
    "\n",
    "itensor.data[:] = 8\n",
    "infer_request.start_async()\n",
    "infer_request.wait()\n",
    "\n",
    "# use numpy array allocated by user as input\n",
    "infer_request.set_input_tensor(0, ov.Tensor(np.ones(compiled_model.input(0).get_shape(), dtype=np.float32)))\n",
    "infer_request.start_async()\n",
    "infer_request.wait()\n",
    "\n",
    "# start_async calls set_input_tensor() internally\n",
    "infer_request.start_async({0:2*np.ones(compiled_model.input(0).get_shape(), dtype=np.float32)})\n",
    "infer_request.wait()\n",
    "\n",
    "# infer returns result dict w/o invoking callback\n",
    "output = infer_request.infer({0:3*np.ones(compiled_model.input(0).get_shape(), dtype=np.float32)})\n",
    "print(output)\n",
    "\n",
    "# infer_request keeps using the same original output tensor for all infer/start_async\n",
    "print(otensor.data)\n",
    "\n",
    "print(\"latency:{} sec\".format(infer_request.latency))\n",
    "\n",
    "profdata = []\n",
    "for pro in infer_request.profiling_info:\n",
    "    profdata.append([pro.node_name, pro.node_type, pro.exec_type, pro.status, pro.real_time.total_seconds()*1e6, pro.cpu_time.total_seconds()*1e6])\n",
    "df = DataFrame(profdata)\n",
    "df.columns=[\"node_name\",\"node_type\",\"exec_type\",\"status\",\"real_time(us)\",\"cpu_time(us)\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiling_info API provides us a insight into the execution, this information is also included in exported model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AsyncInferQueue is a convenient infer_request pool implementation that more friendly to write performance python inference applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_request=1 time_used=0.29945945739746094 sec\n",
      "num_request=16 time_used=0.1412215232849121 sec\n"
     ]
    }
   ],
   "source": [
    "def test_infer_queue(num_request, num_infer):\n",
    "    infer_queue = ov.AsyncInferQueue(compiled_model, num_request)\n",
    "\n",
    "    def callback(request, userdata):\n",
    "        id = userdata\n",
    "\n",
    "        #print(\"infer for id={}\".format(id))\n",
    "        #for k, tensor in enumerate(request.outputs):\n",
    "        #    print(\"  output[{}]:  {} {}\".format(k, tensor, tensor.data))\n",
    "\n",
    "    infer_queue.set_callback(callback)\n",
    "\n",
    "    for i in range(num_infer):\n",
    "        input = ov.Tensor(inode.get_element_type(), inode.get_shape())\n",
    "        input.data[:] = i\n",
    "        infer_queue.start_async({0: input}, userdata=i)\n",
    "\n",
    "    infer_queue.wait_all()\n",
    "\n",
    "for num_request in [1,16]:\n",
    "    t0 = time.time()\n",
    "    test_infer_queue(num_request, 1000)\n",
    "    print(\"num_request={} time_used={} sec\".format(num_request, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Unsolved Questions\n",
    "\n",
    "Python API is good for beginers to learn, especially interactively in Jupyter notebook, but in production we still use C++ API, the good news is they are highly consistent.\n",
    "\n",
    "Python API is also good for experiment with reproducible documentation, so it's absolutely good idea to investigate problem with it."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
