{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with OpenVINO in jupyter notebook\n",
    "\n",
    "Enviroment setup is done as described in [here](./enable_openvino_in_jupyter.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]:CPU --- openvino_intel_cpu_plugin 2022.1 (custom_tq/CVS-74980_3de17de5e388f43df774d3ba55fc50fc9ef91a94)\n"
     ]
    }
   ],
   "source": [
    "from openvino.preprocess import PrePostProcessor\n",
    "import openvino.runtime as ov\n",
    "import numpy as np\n",
    "\n",
    "# this is top object\n",
    "core = ov.Core()\n",
    "\n",
    "for id, dev in enumerate(core.available_devices):\n",
    "    v=core.get_versions(dev)[dev]\n",
    "    print('[{}]:{} --- {} {}.{} ({})'.format(id, dev, v.description, v.major, v.minor, v.build_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description API\n",
    "\n",
    "ov.op/opset8/Model is for describing model using python. but this is not typical usage since openvino do not train NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_node: <Parameter: 'Parameter_16149' ({2,3,7,7}, float)>\n",
      "conv_node:<Convolution: 'Convolution_16151' ({2,1,7,7})>\n",
      "===================\n",
      "<Parameter: 'Parameter_16149' ({2,3,7,7}, float)>\n",
      "<Constant: 'Constant_16150' ({1,3,1,1})>\n",
      "<Convolution: 'Convolution_16151' ({2,1,7,7})>\n",
      "<MaxPool: 'MaxPool_16152' ({2,1,4,4})>\n",
      "<Relu: 'Relu_16153' ({2,1,4,4})>\n",
      "<Result: 'Result_16154' ({2,1,4,4})>\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "input_shape = [2, 3, 7, 7]\n",
    "param_node = ov.op.Parameter(ov.Type.f32, ov.Shape(input_shape))\n",
    "print('param_node: {}'.format(param_node))\n",
    "\n",
    "# convolution weights in shape [out_channels, in_channels, kernel_height, kernel_width]\n",
    "padding_begin = padding_end = [0, 0]\n",
    "conv_kernel = ov.op.Constant(ov.Type.f32, ov.Shape([1,3,1,1]), np.ones([3]))\n",
    "conv_node = ov.opset8.convolution(param_node, conv_kernel, [1, 1], padding_begin, padding_end, [1, 1])\n",
    "\n",
    "# we can see shape inference happens immediatly\n",
    "print('conv_node:{}'.format(conv_node))\n",
    "\n",
    "maxpool_node = ov.opset1.max_pool(conv_node, [2, 2], padding_begin, padding_end, [2, 2], 'ceil')\n",
    "relu_node = ov.opset8.relu(maxpool_node)\n",
    "\n",
    "model = ov.Model(relu_node, [param_node], 'cnntest')\n",
    "\n",
    "print('===================')\n",
    "for op in model.get_ordered_ops():\n",
    "    print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `Model` here is high-level device-independent representation of CNN in openvino, most of the time user get this by converting from models of other NN frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model convert(import) APIs\n",
    "\n",
    "src/bindings/python/src/pyopenvino/core/core.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check supportiveness of a Model\n",
    "\n",
    "Usually user just call compile_model() on a device w/o this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant_16150 is supported by CPU\n",
      "Convolution_16151 is supported by CPU\n",
      "MaxPool_16152 is supported by CPU\n",
      "Parameter_16149 is supported by CPU\n",
      "Relu_16153 is supported by CPU\n",
      "Result_16154 is supported by CPU\n"
     ]
    }
   ],
   "source": [
    "# returns  Pairs a operation name -> a device name supporting this operation.\n",
    "for opname, devname in core.query_model(model, 'CPU').items():\n",
    "    print(\"{} is supported by {}\".format(opname, devname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model on particular device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported model is saved to  /home/hddl/NN-runtimes/OpenVINO/my_model\n",
      "=============runtime model=============\n",
      "<Parameter: 'Parameter_16149' ({2,3,7,7}, float)>\n",
      "<ExecutionNode: 'Constant_16150' ({1,3,1,1})>\n",
      "<ExecutionNode: 'Constant_16150_abcd_Acdb8a_Convolution_16151' ({1,3,1,1})>\n",
      "<ExecutionNode: 'Convolution_16151' ({2,1,7,7})>\n",
      "<ExecutionNode: 'Convolution_16151_aBcd8b_abcd_MaxPool_16152' ({2,1,7,7})>\n",
      "<ExecutionNode: 'MaxPool_16152' ({2,1,4,4})>\n",
      "<Result: 'Result_16154' ({2,1,4,4})>\n"
     ]
    }
   ],
   "source": [
    "compiled_model = core.compile_model(model, 'CPU')\n",
    "\n",
    "# exported model is actually the IR in xml format\n",
    "user_stream = compiled_model.export_model()\n",
    "with open('./my_model', 'wb') as f:\n",
    "    f.write(user_stream)\n",
    "\n",
    "!echo \"exported model is saved to \" `realpath ./my_model`\n",
    "\n",
    "# import_model also returns a compiled model\n",
    "#core.import_model(\"./my_model\",'CPU')\n",
    "\n",
    "# runtime model is internal low-level executable graph for particular device\n",
    "# although still a ngraph function, but it uses different set of op (`ExecutionNode`)\n",
    "# to represent internal OP\n",
    "print(\"=============runtime model=============\")\n",
    "runtime_model = compiled_model.get_runtime_model()\n",
    "for op in runtime_model.get_ordered_ops():\n",
    "    print(op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The APIs and relationships can be visualized as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: openvino models relationship Pages: 1 -->\n<svg width=\"415pt\" height=\"305pt\"\n viewBox=\"0.00 0.00 415.19 305.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 301)\">\n<title>openvino models relationship</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-301 411.1908,-301 411.1908,4 -4,4\"/>\n<!-- Model -->\n<g id=\"node1\" class=\"node\">\n<title>Model</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"79.1908\" cy=\"-192\" rx=\"39.7935\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"79.1908\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Model</text>\n</g>\n<!-- CompiledModel -->\n<g id=\"node2\" class=\"node\">\n<title>CompiledModel</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"139.1908\" cy=\"-105\" rx=\"83.6854\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"139.1908\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">CompiledModel</text>\n</g>\n<!-- Model&#45;&gt;CompiledModel -->\n<g id=\"edge1\" class=\"edge\">\n<title>Model&#45;&gt;CompiledModel</title>\n<path fill=\"none\" stroke=\"#0000ff\" d=\"M72.3547,-173.8582C69.6594,-163.5609 68.4094,-150.8109 74.1908,-141 77.5431,-135.3111 82.1536,-130.458 87.385,-126.3337\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"89.7382,-128.9652 96.0254,-120.4375 85.7925,-123.1832 89.7382,-128.9652\"/>\n<text text-anchor=\"middle\" x=\"133.1908\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compile_model()</text>\n</g>\n<!-- IR -->\n<g id=\"node3\" class=\"node\">\n<title>IR</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"71.1908\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"71.1908\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">IR</text>\n</g>\n<!-- CompiledModel&#45;&gt;IR -->\n<g id=\"edge2\" class=\"edge\">\n<title>CompiledModel&#45;&gt;IR</title>\n<path fill=\"none\" stroke=\"#ff0000\" d=\"M70.2869,-94.5619C41.5303,-88.6666 12.7989,-80.2045 4.1908,-69 -8.6674,-52.2637 14.2873,-38.1839 36.7501,-29.0873\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"38.124,-32.3098 46.2411,-25.5007 35.6495,-25.7617 38.124,-32.3098\"/>\n<text text-anchor=\"middle\" x=\"60.6908\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">.export_model()</text>\n</g>\n<!-- Model(Runtime) -->\n<g id=\"node5\" class=\"node\">\n<title>Model(Runtime)</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"272.1908\" cy=\"-18\" rx=\"85.5853\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"272.1908\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Model(Runtime)</text>\n</g>\n<!-- CompiledModel&#45;&gt;Model(Runtime) -->\n<g id=\"edge6\" class=\"edge\">\n<title>CompiledModel&#45;&gt;Model(Runtime)</title>\n<path fill=\"none\" stroke=\"#ff0000\" d=\"M196.8754,-91.8097C212.0316,-86.4583 227.6339,-79.08 240.1908,-69 248.0924,-62.6569 254.5983,-53.805 259.6324,-45.2537\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"262.8364,-46.6868 264.5138,-36.2256 256.6788,-43.3575 262.8364,-46.6868\"/>\n<text text-anchor=\"middle\" x=\"330.1908\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">.get_runtime_model()</text>\n</g>\n<!-- IR&#45;&gt;CompiledModel -->\n<g id=\"edge5\" class=\"edge\">\n<title>IR&#45;&gt;CompiledModel</title>\n<path fill=\"none\" stroke=\"#0000ff\" d=\"M91.4319,-30.4918C100.2359,-36.7057 110.1301,-44.8459 117.1908,-54 122.5501,-60.9483 126.8791,-69.401 130.2332,-77.4107\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"126.9965,-78.7458 133.8445,-86.83 133.5326,-76.2398 126.9965,-78.7458\"/>\n<text text-anchor=\"middle\" x=\"181.1908\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">import_model()</text>\n</g>\n<!-- IR/ONNX/PDPD -->\n<g id=\"node4\" class=\"node\">\n<title>IR/ONNX/PDPD</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"139.1908\" cy=\"-279\" rx=\"85.5853\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"139.1908\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">IR/ONNX/PDPD</text>\n</g>\n<!-- IR/ONNX/PDPD&#45;&gt;Model -->\n<g id=\"edge4\" class=\"edge\">\n<title>IR/ONNX/PDPD&#45;&gt;Model</title>\n<path fill=\"none\" stroke=\"#0000ff\" d=\"M115.0788,-261.3324C108.9575,-255.9714 102.8136,-249.7077 98.1908,-243 93.3651,-235.998 89.5842,-227.6159 86.7098,-219.6895\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"89.9846,-218.4432 83.5434,-210.0312 83.3329,-220.624 89.9846,-218.4432\"/>\n<text text-anchor=\"middle\" x=\"145.1908\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">read_model()</text>\n</g>\n<!-- IR/ONNX/PDPD&#45;&gt;CompiledModel -->\n<g id=\"edge3\" class=\"edge\">\n<title>IR/ONNX/PDPD&#45;&gt;CompiledModel</title>\n<path fill=\"none\" stroke=\"#0000ff\" d=\"M173.9926,-262.5003C181.2883,-257.3284 188.0259,-250.8638 192.1908,-243 213.4084,-202.9385 213.4084,-181.0615 192.1908,-141 189.5877,-136.0851 185.9797,-131.7168 181.8585,-127.8691\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"183.9668,-125.0727 173.9926,-121.4997 179.5617,-130.5128 183.9668,-125.0727\"/>\n<text text-anchor=\"middle\" x=\"266.1908\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compile_model()</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.files.Source at 0x7f0d0f9c5198>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph, Source\n",
    "g = Digraph(\"openvino models relationship\")\n",
    "g.node(name='Model')\n",
    "g.node(name='CompiledModel')\n",
    "g.node(name='IR')\n",
    "g.node(name='IR/ONNX/PDPD')\n",
    "g.node(name='Model(Runtime)')\n",
    "\n",
    "g.edge('Model','CompiledModel',label='compile_model()', color='blue')\n",
    "g.edge('CompiledModel','IR',label='.export_model()', color='red')\n",
    "g.edge('IR/ONNX/PDPD','CompiledModel',label='compile_model()', color='blue')\n",
    "g.edge('IR/ONNX/PDPD','Model',label='read_model()', color='blue')\n",
    "g.edge('IR','CompiledModel',label='import_model()', color='blue')\n",
    "g.edge('CompiledModel','Model(Runtime)',label='.get_runtime_model()', color='red')\n",
    "\n",
    "Source(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    " - src/bindings/python/src/pyopenvino/core/compiled_model.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check input/output information of the model\n",
    "\n",
    " - src/bindings/python/src/pyopenvino/graph/node_output.hpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[0]: <ConstOutput: names[Parameter_16149] shape{2,3,7,7} type: f32>\n",
      "<class 'openvino.pyopenvino.ConstOutput'>\n",
      "outputs[0]: <ConstOutput: names[] shape{2,1,4,4} type: f32>\n"
     ]
    }
   ],
   "source": [
    "# check inputs & outputs (they are actually graph op)\n",
    "# these are `ov::Output<const ov::Node>` type internally\n",
    "for k, input in enumerate(compiled_model.inputs):\n",
    "    print(\"inputs[{}]: {}\".format(k,input))\n",
    "for k, output in enumerate(compiled_model.outputs):\n",
    "    print(\"outputs[{}]: {}\".format(k,output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronously:  CompiledModel.infer_new_request()\n",
    "\n",
    "Steps:\n",
    " - Create input `ov.Tensor`: src/bindings/python/src/pyopenvino/core/tensor.\n",
    " - Feed data into tensor\n",
    " - invoke `infer_new_request()`\n",
    "\n",
    " This is low-performance API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<ConstOutput: names[] shape{2,1,4,4} type: f32>: array([[[[3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.],\n",
      "         [3., 3., 3., 3.]]],\n",
      "\n",
      "\n",
      "       [[[6., 6., 6., 6.],\n",
      "         [6., 6., 6., 6.],\n",
      "         [6., 6., 6., 6.],\n",
      "         [6., 6., 6., 6.]]]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# get output description of the model\n",
    "inode = compiled_model.input(0)\n",
    "\n",
    "# numpy array can also be input of infer_new_request()\n",
    "# but Tensor is best since it can interpret element_type correctly\n",
    "input = ov.Tensor(inode.get_element_type(), inode.get_shape())\n",
    "\n",
    "# the data member is numpy array wrapper of the tensor\n",
    "# so we can manipulate data using numpy\n",
    "input.data[0,:,:,:] = 1\n",
    "input.data[1,:,:,:] = 2\n",
    "\n",
    "# infer_new_request is synchronous API\n",
    "# the key in input dict can be int/string/opNode\n",
    "result = compiled_model.infer_new_request({0:input})\n",
    "result = compiled_model.infer_new_request({'Parameter_16149':input})\n",
    "result = compiled_model.infer_new_request({compiled_model.input(0):input})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronously:  CompiledModel.infer_new_request()\n",
    "\n",
    " - src/bindings/python/src/pyopenvino/core/infer_request.cpp\n",
    " - src/bindings/python/src/pyopenvino/core/async_infer_queue.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_request=64 time_used=0.16123437881469727 sec\n",
      "num_request=8 time_used=0.11992549896240234 sec\n",
      "num_request=1 time_used=0.20255088806152344 sec\n"
     ]
    }
   ],
   "source": [
    "#infer_request = compiled_model.create_infer_request()\n",
    "#infer_request.start_async()\n",
    "\n",
    "def test_infer_queue(num_request, num_infer):\n",
    "    infer_queue = ov.AsyncInferQueue(compiled_model, num_request)\n",
    "\n",
    "    def callback(request, userdata):\n",
    "        id = userdata\n",
    "\n",
    "        #print(\"infer for id={}\".format(id))\n",
    "        #for k, tensor in enumerate(request.outputs):\n",
    "        #    print(\"  output[{}]:  {} {}\".format(k, tensor, tensor.data))\n",
    "\n",
    "    infer_queue.set_callback(callback)\n",
    "\n",
    "    for i in range(num_infer):\n",
    "        input = ov.Tensor(inode.get_element_type(), inode.get_shape())\n",
    "        input.data[:] = i\n",
    "        infer_queue.start_async({0: input}, userdata=i)\n",
    "\n",
    "    infer_queue.wait_all()\n",
    "\n",
    "import time\n",
    "\n",
    "for num_request in [64,8,1]:\n",
    "    t0 = time.time()\n",
    "    test_infer_queue(num_request, 1000)\n",
    "    print(\"num_request={} time_used={} sec\".format(num_request, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#dir(core)\n",
    "\n",
    "'''\n",
    "'add_extension',\n",
    " 'available_devices',\n",
    " 'compile_model',\n",
    " 'get_config',\n",
    " 'get_metric',\n",
    " 'get_property',\n",
    " 'get_versions',\n",
    " 'import_model',\n",
    " 'query_model',\n",
    " 'read_model',\n",
    " 'register_plugin',\n",
    " 'register_plugins',\n",
    " 'set_config',\n",
    " 'set_property',\n",
    " 'unload_plugin']\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
