{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with OpenVINO in jupyter notebook\n",
    "\n",
    "Enviroment setup is done as described in [here](./enable_openvino_in_jupyter.md)\n",
    "\n",
    " - C++: src/inference/src/ie_core.cpp\n",
    " - Python: src/bindings/python/src/pyopenvino/core/core.cpp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CPU</td>\n",
       "      <td>openvino_intel_cpu_plugin</td>\n",
       "      <td>2022.1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...</td>\n",
       "      <td>SUPPORTED_PROPERTIES</td>\n",
       "      <td>RO</td>\n",
       "      <td>{'SUPPORTED_PROPERTIES': 'RO', 'AVAILABLE_DEVI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...</td>\n",
       "      <td>AVAILABLE_DEVICES</td>\n",
       "      <td>RO</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...</td>\n",
       "      <td>RANGE_FOR_ASYNC_INFER_REQUESTS</td>\n",
       "      <td>RO</td>\n",
       "      <td>(1, 1, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>...</td>\n",
       "      <td>RANGE_FOR_STREAMS</td>\n",
       "      <td>RO</td>\n",
       "      <td>(1, 88)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>...</td>\n",
       "      <td>FULL_DEVICE_NAME</td>\n",
       "      <td>RO</td>\n",
       "      <td>Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>...</td>\n",
       "      <td>OPTIMIZATION_CAPABILITIES</td>\n",
       "      <td>RO</td>\n",
       "      <td>[FP32, FP16, INT8, BIN, IMPORT_EXPORT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>...</td>\n",
       "      <td>NUM_STREAMS</td>\n",
       "      <td>RW</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>...</td>\n",
       "      <td>AFFINITY</td>\n",
       "      <td>RW</td>\n",
       "      <td>(ERROR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>...</td>\n",
       "      <td>INFERENCE_NUM_THREADS</td>\n",
       "      <td>RW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>...</td>\n",
       "      <td>PERF_COUNT</td>\n",
       "      <td>RW</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>...</td>\n",
       "      <td>INFERENCE_PRECISION_HINT</td>\n",
       "      <td>RW</td>\n",
       "      <td>(ERROR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>...</td>\n",
       "      <td>PERFORMANCE_HINT</td>\n",
       "      <td>RW</td>\n",
       "      <td>(ERROR)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>...</td>\n",
       "      <td>PERFORMANCE_HINT_NUM_REQUESTS</td>\n",
       "      <td>RW</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                               1       2  \\\n",
       "0   CPU       openvino_intel_cpu_plugin  2022.1   \n",
       "1   ...            SUPPORTED_PROPERTIES      RO   \n",
       "2   ...               AVAILABLE_DEVICES      RO   \n",
       "3   ...  RANGE_FOR_ASYNC_INFER_REQUESTS      RO   \n",
       "4   ...               RANGE_FOR_STREAMS      RO   \n",
       "5   ...                FULL_DEVICE_NAME      RO   \n",
       "6   ...       OPTIMIZATION_CAPABILITIES      RO   \n",
       "7   ...                     NUM_STREAMS      RW   \n",
       "8   ...                        AFFINITY      RW   \n",
       "9   ...           INFERENCE_NUM_THREADS      RW   \n",
       "10  ...                      PERF_COUNT      RW   \n",
       "11  ...        INFERENCE_PRECISION_HINT      RW   \n",
       "12  ...                PERFORMANCE_HINT      RW   \n",
       "13  ...   PERFORMANCE_HINT_NUM_REQUESTS      RW   \n",
       "\n",
       "                                                    3  \n",
       "0                                                None  \n",
       "1   {'SUPPORTED_PROPERTIES': 'RO', 'AVAILABLE_DEVI...  \n",
       "2                                                  []  \n",
       "3                                           (1, 1, 1)  \n",
       "4                                             (1, 88)  \n",
       "5           Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz  \n",
       "6              [FP32, FP16, INT8, BIN, IMPORT_EXPORT]  \n",
       "7                                                   1  \n",
       "8                                             (ERROR)  \n",
       "9                                                   0  \n",
       "10                                              False  \n",
       "11                                            (ERROR)  \n",
       "12                                            (ERROR)  \n",
       "13                                                  0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openvino.preprocess import PrePostProcessor\n",
    "import openvino.runtime as ov\n",
    "import numpy as np\n",
    "import time\n",
    "from pandas import DataFrame\n",
    "\n",
    "# this is top object\n",
    "core = ov.Core()\n",
    "\n",
    "proplist = []\n",
    "\n",
    "for id, dev in enumerate(core.available_devices):\n",
    "    v=core.get_versions(dev)[dev]\n",
    "    proplist.append([dev, v.description, \"{}.{}\".format(v.major, v.minor, v.build_number)])\n",
    "    SUPPORTED_PROPERTIES = core.get_property(dev, \"SUPPORTED_PROPERTIES\")\n",
    "    for prop, rw in SUPPORTED_PROPERTIES.items():\n",
    "        try:\n",
    "            value = core.get_property(dev, prop)\n",
    "        except:\n",
    "            value = \"(ERROR)\"\n",
    "        proplist.append([\"...\", prop, rw, value])\n",
    "df = DataFrame(proplist)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description API\n",
    "\n",
    "ov.op/opset8/Model is for describing model using python. but this is not typical usage since openvino do not train NN.\n",
    "\n",
    " - src/bindings/python/src/pyopenvino/graph/node.cpp\n",
    " - src/core/src/model.cpp\n",
    " - src/bindings/python/src/pyopenvino/graph/model.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param_node: <Parameter: 'Parameter_10552' ({2,3,7,7}, float)>\n",
      "conv_node:<Convolution: 'Convolution_10554' ({2,1,7,7})>\n",
      "maxpool_node:<MaxPool: 'MaxPool_10555' ({2,1,2,2})>\n",
      "<Model: 'cnntest'\n",
      "inputs[\n",
      "<ConstOutput: names[Parameter_10552] shape{2,3,7,7} type: f32>\n",
      "]\n",
      "outputs[\n",
      "<ConstOutput: names[] shape{8} type: f32>\n",
      "]>\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "input_shape = [2, 3, 7, 7]\n",
    "param_node = ov.op.Parameter(ov.Type.f32, ov.Shape(input_shape))\n",
    "print('param_node: {}'.format(param_node))\n",
    "\n",
    "# convolution weights in shape [out_channels, in_channels, kernel_height, kernel_width]\n",
    "padding_begin = padding_end = [0, 0]\n",
    "conv_kernel = ov.op.Constant(ov.Type.f32, ov.Shape([1,3,1,1]), np.ones([3]))\n",
    "conv_node = ov.opset8.convolution(param_node, conv_kernel, [1, 1], padding_begin, padding_end, [1, 1])\n",
    "\n",
    "# we can see shape inference happens immediatly\n",
    "print('conv_node:{}'.format(conv_node))\n",
    "\n",
    "maxpool_node = ov.opset1.max_pool(conv_node, [4, 4], padding_begin, padding_end, [4, 4], 'ceil')\n",
    "print('maxpool_node:{}'.format(maxpool_node))\n",
    "\n",
    "relu_node = ov.opset8.relu(maxpool_node)\n",
    "reshape_node = ov.opset8.reshape(relu_node, [-1], False)\n",
    "model = ov.Model(reshape_node, [param_node], 'cnntest')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple piece of code for examining the model in a human-friendly way, just like `onnx.helper.printable_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(Parameter_10552):\n",
      "    t1 = Parameter()  #Parameter_10552: {'element_type': 'f32'}\n",
      "    t2 = Constant()  #Constant_10553: {'element_type': 'f32', 'shape': [1, 3, 1, 1]}\n",
      "    t3 = Convolution(t1,t2)  #Convolution_10554: {'strides': [1, 1], 'dilations': [1, 1], 'pads_begin': [0, 0], 'pads_end': [0, 0], 'auto_pad': 'explicit'}\n",
      "    t4 = MaxPool(t3)  #MaxPool_10555: {'strides': [4, 4], 'pads_begin': [0, 0], 'pads_end': [0, 0], 'kernel': [4, 4], 'rounding_type': 'ceil', 'auto_pad': 'explicit'}\n",
      "    t5 = Relu(t4)  #Relu_10556: {}\n",
      "    t6 = Constant()  #Constant_10557: {'element_type': 'i64', 'shape': [1]}\n",
      "    t7 = Reshape(t5,t6)  #Reshape_10558: {'special_zero': False}\n",
      "    t8 = Result(t7)  #Result_10564: {}\n",
      "    return t8\n"
     ]
    }
   ],
   "source": [
    "# print Model in readable text\n",
    "def print_model(model):\n",
    "    out2name = {}\n",
    "    nameid = 1\n",
    "\n",
    "    ilist = [i.get_node().get_name() for i in model.inputs]\n",
    "    print(\"model({}):\".format(\",\".join(ilist)))\n",
    "    for n in model.get_ordered_ops():\n",
    "        # collect output and also allocate output names\n",
    "        returns = []\n",
    "        for out in n.outputs():\n",
    "            varname = \"t{}\".format(nameid)\n",
    "            returns.append(varname)\n",
    "            out2name[out] = varname\n",
    "            nameid += 1\n",
    "        # collect source output names of corresponding inputs\n",
    "        args = [out2name[i.get_source_output()] for i in n.inputs()]\n",
    "\n",
    "        # generate psuedo code\n",
    "        print(\"    {} = {}({})  #{}: {}\".format(\",\".join(returns), \\\n",
    "                        n.get_type_name(),\n",
    "                        \",\".join(args), \\\n",
    "                        n.get_friendly_name(), \\\n",
    "                        n.get_attributes()))\n",
    "    olist = [out2name[i] for i in model.outputs]\n",
    "    print(\"    return {}\".format(\",\".join(olist)))\n",
    "\n",
    "print_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `Model` here is high-level device-independent representation of CNN in openvino, most of the time user get this by converting from models of other NN frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model convert(import) APIs\n",
    "\n",
    "src/bindings/python/src/pyopenvino/core/core.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check supportiveness of a Model\n",
    "\n",
    "Usually user just call compile_model() on a device w/o this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant_10553 is supported by CPU\n",
      "Constant_10557 is supported by CPU\n",
      "Convolution_10554 is supported by CPU\n",
      "MaxPool_10555 is supported by CPU\n",
      "Parameter_10552 is supported by CPU\n",
      "Relu_10556 is supported by CPU\n",
      "Reshape_10558 is supported by CPU\n",
      "Result_10564 is supported by CPU\n"
     ]
    }
   ],
   "source": [
    "# returns  Pairs a operation name -> a device name supporting this operation.\n",
    "for opname, devname in core.query_model(model, 'CPU').items():\n",
    "    print(\"{} is supported by {}\".format(opname, devname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model on particular device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported model is saved to  /home/hddl/NN-runtimes/OpenVINO/my_model\n",
      "=============runtime model=============\n",
      "model(Parameter_13119):\n",
      "    t1 = Parameter()  #Parameter_10552: {'element_type': 'f32'}\n",
      "    t2 = ExecutionNode()  #Constant_10553: {}\n",
      "    t3 = ExecutionNode(t2)  #Constant_10553_abcd_Acdb8a_Convolution_10554: {}\n",
      "    t4 = ExecutionNode(t1,t3)  #Convolution_10554: {}\n",
      "    t5 = ExecutionNode(t4)  #Convolution_10554_aBcd8b_abcd_MaxPool_10555: {}\n",
      "    t6 = ExecutionNode(t5)  #MaxPool_10555: {}\n",
      "    t7 = ExecutionNode()  #Constant_10557: {}\n",
      "    t8 = ExecutionNode(t6,t7)  #Reshape_10558: {}\n",
      "    t9 = Result(t8)  #Result_10564: {}\n",
      "    return t9\n"
     ]
    }
   ],
   "source": [
    "device = 'CPU'\n",
    "# enable PERF_COUNT\n",
    "core.set_property(device, {\"PERF_COUNT\": \"YES\"})\n",
    "compiled_model = core.compile_model(model, device)\n",
    "\n",
    "# exported model is actually the IR in xml format\n",
    "user_stream = compiled_model.export_model()\n",
    "with open('./my_model', 'wb') as f:\n",
    "    f.write(user_stream)\n",
    "\n",
    "!echo \"exported model is saved to \" `realpath ./my_model`\n",
    "\n",
    "# import_model also returns a compiled model\n",
    "#core.import_model(\"./my_model\",'CPU')\n",
    "\n",
    "# runtime model is internal low-level executable graph for particular device\n",
    "# although still a ngraph function, but it uses different set of op (`ExecutionNode`)\n",
    "# to represent internal OP\n",
    "print(\"=============runtime model=============\")\n",
    "runtime_model = compiled_model.get_runtime_model()\n",
    "print_model(runtime_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The APIs and relationships can be visualized as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: openvino models relationship Pages: 1 -->\n<svg width=\"415pt\" height=\"305pt\"\n viewBox=\"0.00 0.00 415.19 305.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 301)\">\n<title>openvino models relationship</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-301 411.1908,-301 411.1908,4 -4,4\"/>\n<!-- Model -->\n<g id=\"node1\" class=\"node\">\n<title>Model</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"79.1908\" cy=\"-192\" rx=\"39.7935\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"79.1908\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Model</text>\n</g>\n<!-- CompiledModel -->\n<g id=\"node2\" class=\"node\">\n<title>CompiledModel</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"139.1908\" cy=\"-105\" rx=\"83.6854\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"139.1908\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">CompiledModel</text>\n</g>\n<!-- Model&#45;&gt;CompiledModel -->\n<g id=\"edge1\" class=\"edge\">\n<title>Model&#45;&gt;CompiledModel</title>\n<path fill=\"none\" stroke=\"#0000ff\" d=\"M72.3547,-173.8582C69.6594,-163.5609 68.4094,-150.8109 74.1908,-141 77.5431,-135.3111 82.1536,-130.458 87.385,-126.3337\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"89.7382,-128.9652 96.0254,-120.4375 85.7925,-123.1832 89.7382,-128.9652\"/>\n<text text-anchor=\"middle\" x=\"133.1908\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compile_model()</text>\n</g>\n<!-- IR -->\n<g id=\"node3\" class=\"node\">\n<title>IR</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"71.1908\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"71.1908\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">IR</text>\n</g>\n<!-- CompiledModel&#45;&gt;IR -->\n<g id=\"edge2\" class=\"edge\">\n<title>CompiledModel&#45;&gt;IR</title>\n<path fill=\"none\" stroke=\"#ff0000\" d=\"M70.2869,-94.5619C41.5303,-88.6666 12.7989,-80.2045 4.1908,-69 -8.6674,-52.2637 14.2873,-38.1839 36.7501,-29.0873\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"38.124,-32.3098 46.2411,-25.5007 35.6495,-25.7617 38.124,-32.3098\"/>\n<text text-anchor=\"middle\" x=\"60.6908\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">.export_model()</text>\n</g>\n<!-- Model(Runtime) -->\n<g id=\"node5\" class=\"node\">\n<title>Model(Runtime)</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"272.1908\" cy=\"-18\" rx=\"85.5853\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"272.1908\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Model(Runtime)</text>\n</g>\n<!-- CompiledModel&#45;&gt;Model(Runtime) -->\n<g id=\"edge6\" class=\"edge\">\n<title>CompiledModel&#45;&gt;Model(Runtime)</title>\n<path fill=\"none\" stroke=\"#ff0000\" d=\"M196.8754,-91.8097C212.0316,-86.4583 227.6339,-79.08 240.1908,-69 248.0924,-62.6569 254.5983,-53.805 259.6324,-45.2537\"/>\n<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"262.8364,-46.6868 264.5138,-36.2256 256.6788,-43.3575 262.8364,-46.6868\"/>\n<text text-anchor=\"middle\" x=\"330.1908\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">.get_runtime_model()</text>\n</g>\n<!-- IR&#45;&gt;CompiledModel -->\n<g id=\"edge5\" class=\"edge\">\n<title>IR&#45;&gt;CompiledModel</title>\n<path fill=\"none\" stroke=\"#0000ff\" d=\"M91.4319,-30.4918C100.2359,-36.7057 110.1301,-44.8459 117.1908,-54 122.5501,-60.9483 126.8791,-69.401 130.2332,-77.4107\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"126.9965,-78.7458 133.8445,-86.83 133.5326,-76.2398 126.9965,-78.7458\"/>\n<text text-anchor=\"middle\" x=\"181.1908\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">import_model()</text>\n</g>\n<!-- IR/ONNX/PDPD -->\n<g id=\"node4\" class=\"node\">\n<title>IR/ONNX/PDPD</title>\n<ellipse fill=\"none\" stroke=\"#000000\" cx=\"139.1908\" cy=\"-279\" rx=\"85.5853\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"139.1908\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">IR/ONNX/PDPD</text>\n</g>\n<!-- IR/ONNX/PDPD&#45;&gt;Model -->\n<g id=\"edge4\" class=\"edge\">\n<title>IR/ONNX/PDPD&#45;&gt;Model</title>\n<path fill=\"none\" stroke=\"#0000ff\" d=\"M115.0788,-261.3324C108.9575,-255.9714 102.8136,-249.7077 98.1908,-243 93.3651,-235.998 89.5842,-227.6159 86.7098,-219.6895\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"89.9846,-218.4432 83.5434,-210.0312 83.3329,-220.624 89.9846,-218.4432\"/>\n<text text-anchor=\"middle\" x=\"145.1908\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">read_model()</text>\n</g>\n<!-- IR/ONNX/PDPD&#45;&gt;CompiledModel -->\n<g id=\"edge3\" class=\"edge\">\n<title>IR/ONNX/PDPD&#45;&gt;CompiledModel</title>\n<path fill=\"none\" stroke=\"#0000ff\" d=\"M173.9926,-262.5003C181.2883,-257.3284 188.0259,-250.8638 192.1908,-243 213.4084,-202.9385 213.4084,-181.0615 192.1908,-141 189.5877,-136.0851 185.9797,-131.7168 181.8585,-127.8691\"/>\n<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"183.9668,-125.0727 173.9926,-121.4997 179.5617,-130.5128 183.9668,-125.0727\"/>\n<text text-anchor=\"middle\" x=\"266.1908\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">compile_model()</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": [
       "<graphviz.files.Source at 0x7fc9d6be4a58>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph, Source\n",
    "g = Digraph(\"openvino models relationship\")\n",
    "g.node(name='Model')\n",
    "g.node(name='CompiledModel')\n",
    "g.node(name='IR')\n",
    "g.node(name='IR/ONNX/PDPD')\n",
    "g.node(name='Model(Runtime)')\n",
    "\n",
    "g.edge('Model','CompiledModel',label='compile_model()', color='blue')\n",
    "g.edge('CompiledModel','IR',label='.export_model()', color='red')\n",
    "g.edge('IR/ONNX/PDPD','CompiledModel',label='compile_model()', color='blue')\n",
    "g.edge('IR/ONNX/PDPD','Model',label='read_model()', color='blue')\n",
    "g.edge('IR','CompiledModel',label='import_model()', color='blue')\n",
    "g.edge('CompiledModel','Model(Runtime)',label='.get_runtime_model()', color='red')\n",
    "\n",
    "Source(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    " - src/bindings/python/src/pyopenvino/core/compiled_model.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check input/output information of the model\n",
    "\n",
    " - src/bindings/python/src/pyopenvino/graph/node_output.hpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[0]: <ConstOutput: names[Parameter_10552] shape{2,3,7,7} type: f32>\n",
      "outputs[0]: <ConstOutput: names[] shape{8} type: f32>\n"
     ]
    }
   ],
   "source": [
    "# check inputs & outputs (they are actually graph op)\n",
    "# these are `ov::Output<const ov::Node>` type internally\n",
    "for k, input in enumerate(compiled_model.inputs):\n",
    "    print(\"inputs[{}]: {}\".format(k,input))\n",
    "for k, output in enumerate(compiled_model.outputs):\n",
    "    print(\"outputs[{}]: {}\".format(k,output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronously:  CompiledModel.infer_new_request()\n",
    "\n",
    "Steps:\n",
    " - Create input `ov.Tensor`: src/bindings/python/src/pyopenvino/core/tensor.\n",
    " - Feed data into tensor\n",
    " - invoke `infer_new_request()`\n",
    "\n",
    " This is low-performance API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{<ConstOutput: names[] shape{8} type: f32>: array([3., 3., 3., 3., 6., 6., 6., 6.], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# get output description of the model\n",
    "inode = compiled_model.input(0)\n",
    "\n",
    "# numpy array can also be input of infer_new_request()\n",
    "# but Tensor is best since it understands element_type while numpy don't\n",
    "input = ov.Tensor(inode.get_element_type(), inode.get_shape())\n",
    "\n",
    "# the data member is numpy array wrapper of the tensor\n",
    "# so we can manipulate data using numpy\n",
    "input.data[0,:,:,:] = 1\n",
    "input.data[1,:,:,:] = 2\n",
    "\n",
    "# infer_new_request is synchronous API\n",
    "# the key in input dict can be int/string/opNode\n",
    "param_name = list(compiled_model.input(0).names)[0]\n",
    "result = compiled_model.infer_new_request({0:input})\n",
    "result = compiled_model.infer_new_request({param_name:input})\n",
    "result = compiled_model.infer_new_request({compiled_model.input(0):input})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronously:  infer_request/async_infer_queue\n",
    "\n",
    " - src/inference/src/cpp/ie_infer_request.cpp\n",
    " - src/bindings/python/src/pyopenvino/core/infer_request.cpp\n",
    " - src/bindings/python/src/pyopenvino/core/async_infer_queue.cpp\n",
    "\n",
    "This concept of infer_request is important:\n",
    " - it has input/output memory allocated by default, user can get a tensor wrapper around them and accessing the memory directly using:\n",
    "    - `get_input_tensor/get_output_tensor`: indexed by port number\n",
    "    - `get_tensor`: indexed by name/port_description/    (internal API: GetBlob)\n",
    " - it also allows users to provide their own memory as desired input/output location:\n",
    "   - `set_input_tensor/set_output_tensor`: indexed by port number\n",
    "   - `set_tensor`: indexed by name/port_description/     (internal API: SetBlob)\n",
    " - it can be triggered in both sync/async way(with optional new input tensor provided), and set call back:\n",
    "   - `start_async`: async\n",
    "   - `infer`:   sync\n",
    " - it tracks latest latency internally:\n",
    "   - `latency`: property\n",
    " - it provides profiling information for each layer:\n",
    "   - `profiling_info`: property (internal API: GetPerformanceCounts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "callback received output tensor:[24. 24. 24. 24. 24. 24. 24. 24.]\n",
      "callback received output tensor:[3. 3. 3. 3. 3. 3. 3. 3.]\n",
      "callback received output tensor:[6. 6. 6. 6. 6. 6. 6. 6.]\n",
      "{<ConstOutput: names[] shape{8} type: f32>: array([9., 9., 9., 9., 9., 9., 9., 9.], dtype=float32)}\n",
      "[9. 9. 9. 9. 9. 9. 9. 9.]\n",
      "latency:0.200811 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node_name</th>\n",
       "      <th>node_type</th>\n",
       "      <th>exec_type</th>\n",
       "      <th>status</th>\n",
       "      <th>real_time(us)</th>\n",
       "      <th>cpu_time(us)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Parameter_10552</td>\n",
       "      <td>Parameter</td>\n",
       "      <td>unknown_FP32</td>\n",
       "      <td>Status.NOT_RUN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Convolution_10554</td>\n",
       "      <td>Convolution</td>\n",
       "      <td>jit_avx2_FP32</td>\n",
       "      <td>Status.EXECUTED</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Relu_10556</td>\n",
       "      <td>Relu</td>\n",
       "      <td>undef</td>\n",
       "      <td>Status.NOT_RUN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Convolution_10554_aBcd8b_abcd_MaxPool_10555</td>\n",
       "      <td>Reorder</td>\n",
       "      <td>ref_any_FP32</td>\n",
       "      <td>Status.EXECUTED</td>\n",
       "      <td>79.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MaxPool_10555</td>\n",
       "      <td>MaxPool</td>\n",
       "      <td>ref_any_FP32</td>\n",
       "      <td>Status.EXECUTED</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reshape_10558</td>\n",
       "      <td>Reshape</td>\n",
       "      <td>unknown_FP32</td>\n",
       "      <td>Status.NOT_RUN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Result_10564</td>\n",
       "      <td>Result</td>\n",
       "      <td>unknown_FP32</td>\n",
       "      <td>Status.NOT_RUN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     node_name    node_type      exec_type  \\\n",
       "0                              Parameter_10552    Parameter   unknown_FP32   \n",
       "1                            Convolution_10554  Convolution  jit_avx2_FP32   \n",
       "2                                   Relu_10556         Relu          undef   \n",
       "3  Convolution_10554_aBcd8b_abcd_MaxPool_10555      Reorder   ref_any_FP32   \n",
       "4                                MaxPool_10555      MaxPool   ref_any_FP32   \n",
       "5                                Reshape_10558      Reshape   unknown_FP32   \n",
       "6                                 Result_10564       Result   unknown_FP32   \n",
       "\n",
       "            status  real_time(us)  cpu_time(us)  \n",
       "0   Status.NOT_RUN            0.0           0.0  \n",
       "1  Status.EXECUTED           51.0          51.0  \n",
       "2   Status.NOT_RUN            0.0           0.0  \n",
       "3  Status.EXECUTED           79.0          79.0  \n",
       "4  Status.EXECUTED           16.0          16.0  \n",
       "5   Status.NOT_RUN            0.0           0.0  \n",
       "6   Status.NOT_RUN            0.0           0.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_request = compiled_model.create_infer_request()\n",
    "def callback1(irq):\n",
    "    print(\"callback received output tensor:{}\".format(irq.get_output_tensor(0).data))\n",
    "\n",
    "infer_request.set_callback(callback1, infer_request)\n",
    "\n",
    "# use input/output memory provided by infer_request\n",
    "itensor = infer_request.get_input_tensor(0)\n",
    "otensor = infer_request.get_output_tensor(0)\n",
    "\n",
    "itensor.data[:] = 8\n",
    "infer_request.start_async()\n",
    "infer_request.wait()\n",
    "\n",
    "# use numpy array allocated by user as input\n",
    "infer_request.set_input_tensor(0, ov.Tensor(np.ones(compiled_model.input(0).get_shape(), dtype=np.float32)))\n",
    "infer_request.start_async()\n",
    "infer_request.wait()\n",
    "\n",
    "# start_async calls set_input_tensor() internally\n",
    "infer_request.start_async({0:2*np.ones(compiled_model.input(0).get_shape(), dtype=np.float32)})\n",
    "infer_request.wait()\n",
    "\n",
    "# infer returns result dict w/o invoking callback\n",
    "output = infer_request.infer({0:3*np.ones(compiled_model.input(0).get_shape(), dtype=np.float32)})\n",
    "print(output)\n",
    "\n",
    "# infer_request keeps using the same original output tensor for all infer/start_async\n",
    "print(otensor.data)\n",
    "\n",
    "print(\"latency:{} sec\".format(infer_request.latency))\n",
    "\n",
    "profdata = []\n",
    "for pro in infer_request.profiling_info:\n",
    "    profdata.append([pro.node_name, pro.node_type, pro.exec_type, pro.status, pro.real_time.total_seconds()*1e6, pro.cpu_time.total_seconds()*1e6])\n",
    "df = DataFrame(profdata)\n",
    "df.columns=[\"node_name\",\"node_type\",\"exec_type\",\"status\",\"real_time(us)\",\"cpu_time(us)\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiling_info API provides us a insight into the execution, this information is also included in exported model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AsyncInferQueue is a convenient infer_request pool implementation that more friendly to write performance python inference applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_request=1 time_used=0.27318501472473145 sec\n",
      "num_request=16 time_used=0.11842226982116699 sec\n"
     ]
    }
   ],
   "source": [
    "def test_infer_queue(num_request, num_infer):\n",
    "    infer_queue = ov.AsyncInferQueue(compiled_model, num_request)\n",
    "\n",
    "    def callback(request, userdata):\n",
    "        id = userdata\n",
    "\n",
    "        #print(\"infer for id={}\".format(id))\n",
    "        #for k, tensor in enumerate(request.outputs):\n",
    "        #    print(\"  output[{}]:  {} {}\".format(k, tensor, tensor.data))\n",
    "\n",
    "    infer_queue.set_callback(callback)\n",
    "\n",
    "    for i in range(num_infer):\n",
    "        input = ov.Tensor(inode.get_element_type(), inode.get_shape())\n",
    "        input.data[:] = i\n",
    "        infer_queue.start_async({0: input}, userdata=i)\n",
    "\n",
    "    infer_queue.wait_all()\n",
    "\n",
    "for num_request in [1,16]:\n",
    "    t0 = time.time()\n",
    "    test_infer_queue(num_request, 1000)\n",
    "    print(\"num_request={} time_used={} sec\".format(num_request, time.time() - t0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
